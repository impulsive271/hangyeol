import pandas as pd
from kiwipiepy import Kiwi
import re
import unicodedata
import json
import os
import google.generativeai as genai
from dotenv import load_dotenv

# .env ë¡œë“œ
load_dotenv()

class SentenceGrader:
    def __init__(self):
        self.is_ready = False
        self.error_msg = ""
        self.use_mock = False 
        self.api_key = os.getenv("GOOGLE_API_KEY")
        self.model = None

        # [ë³€ê²½] Gemini 2.0 Flash Lite ëª¨ë¸ + JSON ëª¨ë“œ ê°•ì œ ì„¤ì •
        if self.api_key:
            try:
                genai.configure(api_key=self.api_key)
                self.model = genai.GenerativeModel(
                    "models/gemini-2.0-flash-lite-preview-02-05",
                    generation_config={"response_mime_type": "application/json"}
                )
            except Exception as e:
                print(f"âš ï¸ Gemini ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

        try:
            # 1. íŒŒì¼ ê²½ë¡œ ë° ë¡œë“œ
            base_dir = os.path.dirname(os.path.abspath(__file__))
            word_path = os.path.join(base_dir, 'word.csv')
            grammar_path = os.path.join(base_dir, 'grammar.csv')

            if not os.path.exists(word_path): raise FileNotFoundError(f"íŒŒì¼ ì—†ìŒ: {word_path}")
            
            self.word_df = pd.read_csv(word_path, encoding='utf-8')
            self.grammar_df = pd.read_csv(grammar_path, encoding='utf-8')
            self.grammar_df['search_related'] = self.grammar_df['ê´€ë ¨í˜•'].fillna('').apply(self._parse_related_forms)

            try: self.analyzer = Kiwi()
            except Exception as e:
                print(f"âš ï¸ Kiwi ë¡œë“œ ì‹¤íŒ¨: {e}")
                self.analyzer = None; self.use_mock = True

            # 'ì´ë‹¤'(#17) ë°ì´í„° ë¯¸ë¦¬ í™•ë³´
            self.ida_entry = None
            try:
                ida_row = self.grammar_df[self.grammar_df['ì „ì²´ ë²ˆí˜¸'] == 17].iloc[0]
                self.ida_entry = {
                    'level': ida_row['ë“±ê¸‰'], 
                    'uid': ida_row['ì „ì²´ ë²ˆí˜¸'], 
                    'desc': ida_row.get('ê¸¸ì¡ì´ë§', ''), 
                    'meaning': ida_row.get('ì˜ë¯¸', '')
                }
            except:
                self.ida_entry = {'level': '1ê¸‰', 'uid': 17, 'desc': 'ì„œìˆ ê²© ì¡°ì‚¬', 'meaning': ''}

            # ë§¤í•‘ í…Œì´ë¸”
            self.pos_map = {
                'NNG': 'N', 'NNP': 'N', 'NR': 'N', 'NP': 'N', 
                'NNB': 'NB', 
                'VV': 'V', 'VA': 'V', 'VX': 'V', 'VCP': 'V', 'VCN': 'V',
                'VV-I': 'V', 'VA-I': 'V', 'VX-I': 'V',
                'MM': 'M', 'MAG': 'MA', 'MAJ': 'MA', 'IC': 'I',
                'EC': 'EC', 'EF': 'EF', 'EP': 'EP', 'ETN': 'ET', 'ETM': 'ET',
                'JKS': 'J', 'JKC': 'J', 'JKG': 'J', 'JKO': 'J', 'JKB': 'J', 
                'JKV': 'J', 'JKQ': 'J', 'JX': 'J', 'JC': 'J'
            }

            self.friendly_pos_map = {
                'NNG': 'ì¼ë°˜ ëª…ì‚¬', 'NNP': 'ê³ ìœ  ëª…ì‚¬', 'NNB': 'ì˜ì¡´ ëª…ì‚¬', 'NR': 'ìˆ˜ì‚¬', 'NP': 'ëŒ€ëª…ì‚¬',
                'VV': 'ë™ì‚¬', 'VA': 'í˜•ìš©ì‚¬', 'VX': 'ë³´ì¡° ìš©ì–¸', 'VCP': 'ê¸ì • ì§€ì •ì‚¬(ì´ë‹¤)', 'VCN': 'ë¶€ì • ì§€ì •ì‚¬',
                'MM': 'ê´€í˜•ì‚¬', 'MAG': 'ì¼ë°˜ ë¶€ì‚¬', 'MAJ': 'ì ‘ì† ë¶€ì‚¬', 'IC': 'ê°íƒ„ì‚¬',
                'JKS': 'ì£¼ê²© ì¡°ì‚¬', 'JKC': 'ë³´ê²© ì¡°ì‚¬', 'JKG': 'ê´€í˜•ê²© ì¡°ì‚¬', 'JKO': 'ëª©ì ê²© ì¡°ì‚¬',
                'JKB': 'ë¶€ì‚¬ê²© ì¡°ì‚¬', 'JKV': 'í˜¸ê²© ì¡°ì‚¬', 'JKQ': 'ì¸ìš©ê²© ì¡°ì‚¬', 'JX': 'ë³´ì¡°ì‚¬', 'JC': 'ì ‘ì† ì¡°ì‚¬',
                'EP': 'ì„ ì–´ë§ ì–´ë¯¸', 'EF': 'ì¢…ê²° ì–´ë¯¸', 'EC': 'ì—°ê²° ì–´ë¯¸', 'ETN': 'ëª…ì‚¬í˜• ì „ì„± ì–´ë¯¸', 'ETM': 'ê´€í˜•í˜• ì „ì„± ì–´ë¯¸',
                'XPN': 'ì²´ì–¸ ì ‘ë‘ì‚¬', 'XSN': 'ëª…ì‚¬ íŒŒìƒ ì ‘ë¯¸ì‚¬', 'XSV': 'ë™ì‚¬ íŒŒìƒ ì ‘ë¯¸ì‚¬', 'XSA': 'í˜•ìš©ì‚¬ íŒŒìƒ ì ‘ë¯¸ì‚¬',
                'XR': 'ì–´ê·¼', 'SF': 'ë§ˆì¹¨í‘œ', 'SP': 'ì‰¼í‘œ', 'SS': 'ë”°ì˜´í‘œ/ê´„í˜¸', 'SE': 'ì¤„ì„í‘œ', 'SO': 'ë¶™ì„í‘œ', 'SW': 'ê¸°íƒ€ ê¸°í˜¸'
            }

            self._build_lookup_tables()
            self.is_ready = True
        except Exception as e:
            self.error_msg = str(e); print(f"ì´ˆê¸°í™” ì˜¤ë¥˜: {self.error_msg}")

    def _clean_key(self, key_str):
        # 1. ë¬¸ìì—´ ë³€í™˜
        key = str(key_str)
        # 2. ìëª¨ ë¶„ë¦¬ ë³´ì •
        key = key.replace('á†¯', 'ã„¹').replace('á†«', 'ã„´').replace('á†¸', 'ã…‚')
        key = key.replace('á†·', 'ã…').replace('á†¼', 'ã…‡').replace('á†¨', 'ã„±')
        # 3. ê¸°ë³¸ ë¬¸ì¥ë¶€í˜¸ ì œê±°
        key = key.replace('.', '').replace('-', '').replace('â€“', '').replace('~', '').replace('"', '').replace("'", '').strip()
        # 4. ì‚¬ì „ í‘œê¸° êµ°ë”ë”ê¸° ì œê±°
        key = re.sub(r'[0-9]+\([0-9]+\)', '', key) # -ë‹¤ê°€1(1) -> ë‹¤ê°€
        key = re.sub(r'\([0-9]+\)', '', key)       # (1) ì œê±°
        key = re.sub(r'[0-9]+$', '', key)          # ë ìˆ«ì ì œê±°
        return unicodedata.normalize('NFKC', key).strip()

    def _parse_related_forms(self, raw_str):
        if not raw_str: return []
        clean_str = re.sub(r'<[^>]+>', ' ', str(raw_str))
        return [item.strip() for item in re.split(r'[,./]', clean_str) if item.strip()]

    def _build_lookup_tables(self):
        # 1. ë‹¨ì–´ ì§€ë„
        self.word_map = {}
        for _, row in self.word_df.fillna('').iterrows():
            pos_str = str(row['í’ˆì‚¬'])
            target_pos_keys = []
            if 'ì˜ì¡´ëª…ì‚¬' in pos_str: target_pos_keys.append('NB')
            if any(x in pos_str for x in ['ëª…ì‚¬', 'ëŒ€ëª…ì‚¬', 'ìˆ˜ì‚¬']) and 'ì˜ì¡´ëª…ì‚¬' not in pos_str:
                target_pos_keys.append('N')
            if any(x in pos_str for x in ['ë™ì‚¬', 'í˜•ìš©ì‚¬']): target_pos_keys.append('V')
            if 'ê´€í˜•ì‚¬' in pos_str: target_pos_keys.append('M')
            if 'ë¶€ì‚¬' in pos_str: target_pos_keys.append('MA')
            if 'ê°íƒ„ì‚¬' in pos_str: target_pos_keys.append('I')
            if not target_pos_keys: target_pos_keys.append(self.pos_map.get(pos_str, 'ETC'))

            raw_words = re.split(r'[?/]', str(row['ì–´íœ˜']))
            for word in raw_words:
                cleaned = self._clean_key(word)
                if cleaned:
                    data = {'level': row['ë“±ê¸‰'], 'uid': row['ì „ì²´ ë²ˆí˜¸'], 'desc': row['ê¸¸ì¡ì´ë§'], 'raw_pos': row['í’ˆì‚¬'], 'is_main': True}
                    for p_key in target_pos_keys:
                        if (cleaned, p_key) not in self.word_map: self.word_map[(cleaned, p_key)] = []
                        is_duplicate = False
                        for existing in self.word_map[(cleaned, p_key)]:
                            if existing['uid'] == data['uid']:
                                is_duplicate = True; break
                        if not is_duplicate:
                            self.word_map[(cleaned, p_key)].append(data)

        # 2. ë¬¸ë²•/í‘œí˜„ ì§€ë„
        self.grammar_map = {}
        self.idiom_map = {} 

        def get_grammar_pos_keys(class_str):
            keys = []
            if 'ì—°ê²°ì–´ë¯¸' in class_str: keys.append('EC')
            if 'ì¢…ê²°ì–´ë¯¸' in class_str: keys.append('EF')
            if 'ì„ ì–´ë§ì–´ë¯¸' in class_str: keys.append('EP')
            if 'ì „ì„±ì–´ë¯¸' in class_str: keys.append('ET')
            if 'ì¡°ì‚¬' in class_str or 'ë³´ì¡°ì‚¬' in class_str: keys.append('J')
            if 'ì˜ì¡´ëª…ì‚¬' in class_str: keys.append('NB')
            elif 'ëª…ì‚¬' in class_str: keys.append('N')
            return keys

        def register_grammar(key, data_dict, is_main=True):
            if key not in self.grammar_map: self.grammar_map[key] = []
            entry = data_dict.copy()
            entry['is_main'] = is_main 
            if not any(d['uid'] == entry['uid'] for d in self.grammar_map[key]):
                self.grammar_map[key].append(entry)

        def register_idiom(raw_pattern, data_dict):
            clean_pat_str = raw_pattern.replace('-', '').replace('~', '').replace('(ìœ¼)', '').strip()
            if not clean_pat_str: return
            pattern_chunks = clean_pat_str.split()
            valid_tokens = []
            try:
                for chunk in pattern_chunks:
                    res = self.analyzer.analyze(chunk)
                    tokens = res[0][0]
                    for idx, t in enumerate(tokens):
                        if chunk == pattern_chunks[-1] and idx == len(tokens) - 1 and t.form == 'ë‹¤' and t.tag == 'EF':
                            continue
                        valid_tokens.append(self._clean_key(t.form))
                if len(valid_tokens) >= 2:
                    start_key = valid_tokens[0]
                    rest_seq = valid_tokens[1:]
                    if start_key not in self.idiom_map: self.idiom_map[start_key] = []
                    exists = False
                    for existing in self.idiom_map[start_key]:
                        if existing['sequence'] == rest_seq and existing['data']['uid'] == data_dict['uid']:
                            exists = True; break
                    if not exists:
                        entry = data_dict.copy()
                        entry['is_main'] = True
                        self.idiom_map[start_key].append({'sequence': rest_seq, 'data': entry, 'full_text': raw_pattern})
            except: pass

        for df_source in [self.grammar_df]: 
            for _, row in df_source.fillna('').iterrows():
                data = {'level': row['ë“±ê¸‰'], 'uid': row['ì „ì²´ ë²ˆí˜¸'], 'desc': row.get('ê¸¸ì¡ì´ë§', ''), 'meaning': row.get('ì˜ë¯¸', ''), 'class': str(row['ë¶„ë¥˜'])}
                main_form = str(row['ëŒ€í‘œí˜•']).strip()
                if ' ' in main_form or 'í‘œí˜„' in data['class']: register_idiom(main_form, data)
                
                class_str = str(row['ë¶„ë¥˜'])
                pos_keys = get_grammar_pos_keys(class_str)
                if 'ì´ë‹¤' in main_form and 'ì¡°ì‚¬' in class_str: cleaned_main = 'ì´ë‹¤'
                else: cleaned_main = self._clean_key(main_form)
                
                if cleaned_main:
                    for pk in pos_keys: register_grammar((cleaned_main, pk), data, is_main=True)
                
                for rel_form in row['search_related']:
                    if ' ' in rel_form or 'í‘œí˜„' in data['class']: register_idiom(rel_form, data)
                    cleaned_rel = self._clean_key(rel_form)
                    if cleaned_rel:
                        for pk in pos_keys: register_grammar((cleaned_rel, pk), data, is_main=False)

        for k in self.idiom_map:
            self.idiom_map[k].sort(key=lambda x: len(x['sequence']), reverse=True)

    # -------------------------------------------------------------------------
    # [ìˆ˜ì •ë¨] AI íŒë… ìš”ì²­ (JSON í¬ë§· ê°•ì œ + ì¸ë±ìŠ¤ í‚¤ ì‚¬ìš©)
    # -------------------------------------------------------------------------
    def _disambiguate_with_ai(self, sentence, ambiguous_items):
        if not self.model or not ambiguous_items: return {}, "AI ë¯¸ì‚¬ìš©"
        
        # 1. í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""
        ë‹¹ì‹ ì€ í•œêµ­ì–´ ì–´íœ˜ íŒë…ê¸°ì…ë‹ˆë‹¤. ì•„ë˜ ë¬¸ë§¥ì„ ë³´ê³  ë™ìŒì´ì˜ì–´ ì¤‘ ê°€ì¥ ì ì ˆí•œ ì˜ë¯¸ë¥¼ ê³ ë¥´ì„¸ìš”.
        ë¬¸ë§¥: "{sentence}"
        
        [íŒë… ëŒ€ìƒ ëª©ë¡]
        """
        for i, item in enumerate(ambiguous_items):
            options_str = ", ".join([f"(ID:{cand['uid']}) {cand['desc']}" for cand in item['candidates']])
            prompt += f"[{i}] ë‹¨ì–´: '{item['word']}' -> í›„ë³´: [{options_str}]\n"
            
        prompt += """
        [ì¶œë ¥ ê·œì¹™]
        1. ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”. (ë§ˆí¬ë‹¤ìš´ ì—†ì´)
        2. KeyëŠ” ìœ„ ëª©ë¡ì˜ [ë²ˆí˜¸]ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. (ì˜ˆ: "0", "1")
        3. ValueëŠ” ì„ íƒí•œ ID ê°’ë§Œ ë„£ìœ¼ì„¸ìš”.
        4. ì˜ˆì‹œ: {"0": "272", "1": "677"}
        """
        
        raw_response = ""
        try:
            response = self.model.generate_content(prompt)
            raw_response = response.text
            
            # 2. ê²°ê³¼ íŒŒì‹± (ë§ˆí¬ë‹¤ìš´ ì œê±° ë° JSON ë³€í™˜)
            clean_json_str = raw_response.replace('```json', '').replace('```', '').strip()
            
            # í˜¹ì‹œ ëª¨ë¥¼ trailing comma ë“± ì‚¬ì†Œí•œ ì˜¤ë¥˜ ë°©ì§€
            if clean_json_str.endswith(',') or clean_json_str.endswith(',}'): 
                 clean_json_str = clean_json_str.rstrip(',}') + "}"
                 
            ai_data = json.loads(clean_json_str)
            return ai_data, raw_response

        except Exception as e:
            error_msg = f"Error: {e} | Raw: {raw_response}"
            return {}, error_msg

    def get_sentence_grade(self, sentence: str):
        if not self.is_ready: return "íŒë… ë¶ˆê°€", [], "ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨"
        if self.use_mock or not self.analyzer: return "ë¶„ì„ ë¶ˆê°€", [], "Kiwi ë¡œë“œ ì‹¤íŒ¨"
        try:
            res = self.analyzer.analyze(sentence)
            tokens = res[0][0]
        except Exception as e: return "ë¶„ì„ ì—ëŸ¬", [], f"Kiwi ë¶„ì„ ì˜¤ë¥˜: {str(e)}"

        max_level = 0; analysis_data = []; debug_lines = []
        ambiguous_items = [] 
        
        debug_lines.append(f"ì…ë ¥: {sentence}")
        
        i = 0
        while i < len(tokens):
            token = tokens[i]
            form = token.form; tag = token.tag; form_clean = self._clean_key(form)
            
            # 0. í‘œí˜„ íŒ¨í„´ ë§¤ì¹­
            idiom_matched = False
            if form_clean in self.idiom_map:
                candidates = self.idiom_map[form_clean]
                for cand in candidates:
                    seq = cand['sequence']
                    if i + len(seq) >= len(tokens): continue
                    match = True
                    matched_tokens_forms = [form]
                    for offset, target_stem in enumerate(seq):
                        next_t = tokens[i + 1 + offset]
                        next_clean = self._clean_key(next_t.form)
                        if next_clean != target_stem: match = False; break
                        matched_tokens_forms.append(next_t.form)
                    
                    if match:
                        data = cand['data']
                        full_pattern_text = "+".join(matched_tokens_forms)
                        debug_lines.append(f"ğŸ§© í‘œí˜„ ë°œê²¬: {full_pattern_text} -> {data['desc']} (#{data['uid']})")
                        level_str = data['level']
                        if level_str:
                            try: max_level = max(max_level, int(re.sub(r'[^0-9]', '', str(level_str))))
                            except: pass
                        analysis_data.append({
                            "form": full_pattern_text, "tag_code": "Expression", "tag_name": "ë¬¸ë²•ì  í‘œí˜„",
                            "level": level_str, "id": f"í‘œí˜„#{data['uid']}", "desc": data['desc']
                        })
                        i += (1 + len(seq))
                        idiom_matched = True; break
            if idiom_matched: continue

            # [VCP ì ˆëŒ€ ìš°ì„ ]
            if tag.startswith('VCP'):
                final_cand = self.ida_entry
                level_str = final_cand['level']
                debug_lines.append(f"ğŸ”’ ì§€ì •ì‚¬(VCP) ê°•ì œ ë§¤í•‘: ì´ë‹¤ -> {level_str} (#{final_cand['uid']})")
                if level_str:
                    try: max_level = max(max_level, int(re.sub(r'[^0-9]', '', str(level_str))))
                    except: pass
                analysis_data.append({
                    "form": form, "tag_code": tag, "tag_name": self.friendly_pos_map.get(tag, tag),
                    "level": level_str, "id": f"ë¬¸ë²•#{final_cand['uid']}", "desc": final_cand['desc']
                })
                i += 1; continue 

            # 1. ë‹¨ì–´ ë³‘í•©
            if i + 1 < len(tokens):
                next_token = tokens[i+1]
                curr_pos_type = self.pos_map.get(tag, 'ETC')
                next_pos_type = self.pos_map.get(next_token.tag, 'ETC')
                
                if curr_pos_type in ['N', 'NB'] and next_pos_type in ['N', 'NB']:
                    combined_form = form_clean + self._clean_key(next_token.form)
                    if (combined_form, 'N') in self.word_map:
                        merged_cands = self.word_map[(combined_form, 'N')]
                        
                        # ëŒ€í‘œí˜• ìš°ì„  í•„í„°ë§
                        main_cands = [c for c in merged_cands if c.get('is_main', False)]
                        if main_cands: merged_cands = main_cands

                        if len(merged_cands) > 1:
                            ambiguous_items.append({'index': len(analysis_data), 'word': combined_form, 'candidates': merged_cands})
                        
                        final_cand = merged_cands[0] 
                        level_str = final_cand['level']
                        debug_lines.append(f"ğŸ”„ ë³‘í•© ì„±ê³µ: {combined_form} (N) -> {level_str}")
                        if level_str:
                            try: max_level = max(max_level, int(re.sub(r'[^0-9]', '', str(level_str))))
                            except: pass
                        analysis_data.append({
                            "form": combined_form, "tag_code": f"{tag}+{next_token.tag}", "tag_name": "ë³µí•©ì–´",
                            "level": level_str, "id": f"ë‹¨ì–´#{final_cand['uid']}", "desc": final_cand['desc']
                        })
                        i += 2; continue

            # 2. ë‹¨ì¼ í† í° ì²˜ë¦¬
            source_type = ""; search_key = ""; candidates = []
            pos_key = self.pos_map.get(tag, 'ETC')

            if tag in ['XSV', 'XSA'] and form_clean == 'í•˜':
                source_type = "ì ‘ë¯¸ì‚¬"; candidates = [{'level': '1ê¸‰', 'uid': 'Sys', 'desc': 'íŒŒìƒ ì ‘ë¯¸ì‚¬', 'is_main': True}]
            elif tag.startswith('J') or tag.startswith('E'):
                source_type = "ë¬¸ë²•"
                if (form_clean, pos_key) in self.grammar_map:
                    candidates = self.grammar_map[(form_clean, pos_key)]
                    search_key = f"({form_clean}, {pos_key})"
                else:
                    fallback_key = 'J' if tag.startswith('J') else 'E'
                    if (form_clean, fallback_key) in self.grammar_map:
                        candidates = self.grammar_map[(form_clean, fallback_key)]
                        search_key = f"({form_clean}, {fallback_key})"
            else:
                source_type = "ë‹¨ì–´"
                target = form_clean + 'ë‹¤' if pos_key == 'V' and not form_clean.endswith('ë‹¤') else form_clean
                search_key = f"({target}, {pos_key})"
                word_candidates = self.word_map.get((target, pos_key), [])
                grammar_candidates = []
                if (target, pos_key) in self.grammar_map:
                    grammar_candidates = self.grammar_map[(target, pos_key)]
                candidates = word_candidates + grammar_candidates

            final_level = "-"; final_id = ""; final_desc = ""
            if candidates:
                # ëŒ€í‘œí˜• ìš°ì„ 
                main_cands = [c for c in candidates if c.get('is_main', False)]
                if main_cands: candidates = main_cands

                if len(candidates) > 1:
                     ambiguous_items.append({'index': len(analysis_data), 'word': target, 'candidates': candidates})
                
                candidates.sort(key=lambda x: x['level'])
                sel = candidates[0]
                final_level = sel['level']; final_id = sel['uid']; final_desc = sel.get('desc', '') or sel.get('meaning', '')
                debug_lines.append(f"['{form}'({tag})] -> í‚¤:{search_key} -> ê²°ê³¼:{final_level} (#{final_id})")
                if final_level:
                    try: max_level = max(max_level, int(re.sub(r'[^0-9]', '', str(final_level))))
                    except: pass
            else:
                debug_lines.append(f"['{form}'({tag})] -> ê²€ìƒ‰ ì‹¤íŒ¨ (X)")

            analysis_data.append({
                "form": form, "tag_code": tag, "tag_name": self.friendly_pos_map.get(tag, tag),
                "level": final_level, "id": f"{source_type}#{final_id}" if final_id else "-",
                "desc": final_desc
            })
            i += 1

        # ---------------------------------------------------------------------
        # [ìˆ˜ì •ë¨] AI ê²°ê³¼ ë°˜ì˜ ë¡œì§ (ì¸ë±ìŠ¤ or ë‹¨ì–´ ë§¤ì¹­)
        # ---------------------------------------------------------------------
        if ambiguous_items and self.model:
            debug_lines.append(f"ğŸ¤– AI ë™ìŒì´ì˜ì–´ íŒë… ì‹œì‘ ({len(ambiguous_items)}ê±´)...")
            ai_decisions, raw_log = self._disambiguate_with_ai(sentence, ambiguous_items)
            
            # [ë””ë²„ê·¸] AIê°€ ì‹¤ì œë¡œ ë±‰ì€ ê°’ì„ ë¡œê·¸ì— ì°ì–´ í™•ì¸ (í•„ìš”ì‹œ ì£¼ì„ í•´ì œ)
            # debug_lines.append(f"ğŸ“ AI Raw: {raw_log}")

            for i, item in enumerate(ambiguous_items):
                key_idx = str(i)        # "0"
                word_key = item['word'] # "ë°°"
                target_idx = item['index']
                
                # [í•µì‹¬] 1ìˆœìœ„: ì¸ë±ìŠ¤ë¡œ ì°¾ê¸° / 2ìˆœìœ„: ë‹¨ì–´ë¡œ ì°¾ê¸°
                selected_uid = None
                
                # 1. ì¸ë±ìŠ¤ ë§¤ì¹­ ("0": "123")
                if key_idx in ai_decisions:
                    selected_uid = str(ai_decisions[key_idx])
                
                # 2. ë‹¨ì–´ ë§¤ì¹­ ("ë°°": "123") - AIê°€ ì§€ì‹œ ì–´ê¸°ê³  ë‹¨ì–´ ì¼ì„ ë•Œ ëŒ€ë¹„
                elif word_key in ai_decisions:
                    selected_uid = str(ai_decisions[word_key])
                
                if selected_uid:
                    # í›„ë³´êµ° ë‚´ì—ì„œ í•´ë‹¹ UID ì°¾ê¸°
                    found = next((c for c in item['candidates'] if str(c['uid']) == selected_uid), None)
                    if found:
                        analysis_data[target_idx]['level'] = found['level']
                        analysis_data[target_idx]['id'] = f"ë‹¨ì–´#{found['uid']}" 
                        analysis_data[target_idx]['desc'] = f"ğŸ¤– {found['desc']}" 
                        debug_lines.append(f"âœ… AI êµì • [{item['word']}]: {found['desc']} (#{selected_uid})")
                        
                        try: 
                            new_lvl = int(re.sub(r'[^0-9]', '', str(found['level'])))
                            max_level = max(max_level, new_lvl)
                        except: pass
                    else:
                        debug_lines.append(f"âš ï¸ ID ë¶ˆì¼ì¹˜: AIê°€ ì—†ëŠ” ID({selected_uid}) ë°˜í™˜")
                else:
                    debug_lines.append(f"âš ï¸ AI ì‘ë‹µ ëˆ„ë½ [{i}]: {item['word']}")

        final_grade = f"{max_level}ê¸‰" if max_level > 0 else "íŒë³„ ë¶ˆê°€"
        return final_grade, analysis_data, "\n".join(debug_lines)

    # (ì´í•˜ search_keyword, generate_ai_sentence ë“±ì€ ê¸°ì¡´ê³¼ ë™ì¼)
    def search_keyword(self, query, search_type):
        if not query or not self.is_ready: return []
        results = []
        def normalize(text):
            if not isinstance(text, str): return ""
            return re.sub(r'[\s\-\~\(\)\[\]\.\?\/ã†]', '', text)
        try:
            if search_type == "word":
                norm_query = normalize(query)
                mask = self.word_df['ì–´íœ˜'].astype(str).apply(normalize).str.contains(norm_query, na=False)
                df = self.word_df[mask].head(10)
                for _, row in df.fillna('').iterrows():
                    results.append({"text": row['ì–´íœ˜'], "grade": row['ë“±ê¸‰'], "desc": str(row['ê¸¸ì¡ì´ë§']), "pos": row['í’ˆì‚¬'], "meaning": ""})
            else:
                norm_query = normalize(query)
                search_candidates = {norm_query}
                target_endings = ['ë‹¤', 'ëŠ”', 'ì€', 'ã„´', 'ì„', 'ã„¹', 'ìš”', 'ì£ ', 'ë‹ˆ', 'ë©´']
                if len(norm_query) >= 2:
                    for end in target_endings:
                        if norm_query.endswith(end):
                            stem = norm_query[:-len(end)]
                            if len(stem) > 0: search_candidates.add(stem)
                            break 
                def check_match(row_text):
                    if not row_text: return False
                    norm_target = normalize(str(row_text)) 
                    for candidate in search_candidates:
                        if candidate in norm_target: return True
                    norm_target_stem = norm_target
                    if norm_target.endswith('ë‹¤'): norm_target_stem = norm_target[:-1]
                    for candidate in search_candidates:
                        if len(norm_target_stem) >= 2 and norm_target_stem in candidate: return True
                    return False
                main_mask = self.grammar_df['ëŒ€í‘œí˜•'].apply(check_match)
                related_mask = self.grammar_df['search_related'].apply(lambda items: any(check_match(item) for item in items))
                final_mask = main_mask | related_mask
                df = self.grammar_df[final_mask].head(10)
                for _, row in df.fillna('').iterrows():
                    results.append({"text": row['ëŒ€í‘œí˜•'], "grade": row['ë“±ê¸‰'], "desc": str(row.get('ê¸¸ì¡ì´ë§', '')), "pos": row['ë¶„ë¥˜'], "related": ", ".join(row['search_related']), "meaning": str(row.get('ì˜ë¯¸', ''))})
        except Exception as e: print(f"ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return results

    def generate_ai_sentence(self, model, grades, keyword, hint=""):
        prompt = "í•œêµ­ì–´ êµìœ¡ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\në‹¤ìŒ ì¡°ê±´ì— ë§ì¶° í•™ìŠµìš© ì˜ˆë¬¸ì„ ë‹¨ í•˜ë‚˜ë§Œ ì‘ì„±í•˜ì„¸ìš”.\n"
        if grades:
            valid = [int(g) for g in grades if g.isdigit()] if "all" not in grades else []
            prompt += f"- ë‚œì´ë„: TOPIK {max(valid)}ê¸‰ ìˆ˜ì¤€\n" if valid else "- ë‚œì´ë„: ì´ˆ~ê³ ê¸‰ ìì—°ìŠ¤ëŸ½ê²Œ\n"
        if keyword:
            hint_str = f" (ë¬¸ë§¥ íŒíŠ¸: {hint})" if hint and hint != 'nan' else ""
            prompt += f"- í•„ìˆ˜ í¬í•¨ ë‹¨ì–´: '{keyword}'{hint_str}\n  * ì£¼ì˜: ë°˜ë“œì‹œ í¬í•¨í•  ê²ƒ.\n"
        prompt += "\n[ì¶œë ¥ ì œì•½ì‚¬í•­]\n1. ì„¤ëª… ê¸ˆì§€, ì˜¤ì§ ì˜ˆë¬¸ 1ê°œë§Œ ì¶œë ¥.\n2. ë§ˆí¬ë‹¤ìš´ ì‚¬ìš© ê¸ˆì§€.\n3. ë°˜ë“œì‹œ ë§ˆì¹¨í‘œë¡œ ëë‚¼ ê²ƒ."
        try:
            return model.generate_content(prompt).text.strip().replace("**", "").replace('"', "")
        except Exception as e: return f"ì˜¤ë¥˜: {str(e)}"

    # ì¸ì(argument)ì— user_prompt="" ì¶”ê°€
    def generate_quiz_item(self, model, target, level, quiz_type, context_sentence, user_prompt=""):
        type_desc = "ì–‘ìíƒì¼(Binary Choice)" if quiz_type == 'binary' else "4ì§€ì„ ë‹¤(Multiple Choice)"
        
        # [NEW] ì‚¬ìš©ì ìš”ì²­ì´ ìˆì„ ê²½ìš° í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€í•  í…ìŠ¤íŠ¸
        custom_instruction = ""
        if user_prompt:
            custom_instruction = f"\n[ì‚¬ìš©ì íŠ¹ë³„ ìš”ì²­ì‚¬í•­]: {user_prompt} (ì´ ìš”ì²­ì„ ìµœìš°ì„ ìœ¼ë¡œ ë°˜ì˜í•  ê²ƒ)\n"

        if context_sentence:
            clean_target = target.split(' (')[0] if '(' in target else target
            prompt = f"""ë‹¹ì‹ ì€ í•œêµ­ì–´ ì„ ìƒë‹˜ì…ë‹ˆë‹¤.
ì›ë¬¸: "{context_sentence}"
ì •ë‹µ: "{clean_target}"
ìœ í˜•: {type_desc}
ë‚œì´ë„: {level}ê¸‰
{custom_instruction}
ì§€ì‹œ: ì •ë‹µì„ ë¹ˆì¹¸(____)ìœ¼ë¡œ ë§Œë“¤ê³  í€´ì¦ˆ ìƒì„±.
ì¶œë ¥ í¬ë§·(JSON): {{"question_text": "...", "options": ["..."], "answer_index": 0, "explanation": "..."}}"""
        else:
            prompt = f"""í•œêµ­ì–´ ë¬¸ì œ ì¶œì œìì…ë‹ˆë‹¤.
ë‹¨ì–´: '{target}' í™œìš©
ë‚œì´ë„: {level}ê¸‰
ìœ í˜•: {type_desc}
{custom_instruction}
ì§€ì‹œ: ìœ„ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ë¬¸ì œ ìƒì„±.
ì¶œë ¥ í¬ë§·(JSON): {{"question_text": "...", "options": ["..."], "answer_index": 0, "explanation": "..."}}"""
            
        try:
            return json.loads(model.generate_content(prompt).text.strip().replace("```json", "").replace("```", ""))
        except Exception as e: return {"error": "AI ìƒì„± ì‹¤íŒ¨", "details": str(e)}