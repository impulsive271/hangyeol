import re
import json
from services.grade_database import GradeDatabase

class GradeProfiler:
    def __init__(self, data_service: GradeDatabase):
        self.data = data_service
        self.debug_lines = []

    def _disambiguate_with_ai(self, client, model_name, sentence, ambiguous_items):
        if not client or not ambiguous_items: return {}, "AI ë¯¸ì‚¬ìš©"
        
        prompt = f"""
        ë‹¹ì‹ ì€ í•œêµ­ì–´ ì–´íœ˜ ë¶„ì„ê¸°ì…ë‹ˆë‹¤. ì•„ë˜ ë¬¸ë§¥ì„ ë³´ê³  ë™ìŒì´ì˜ì–´ ì¤‘ ê°€ì¥ ì ì ˆí•œ ì˜ë¯¸ë¥¼ ê³ ë¥´ì„¸ìš”.
        ë¬¸ë§¥: "{sentence}"
        
        [ë¶„ì„ ëŒ€ìƒ ëª©ë¡]
        """
        for i, item in enumerate(ambiguous_items):
            options_str = ", ".join([f"(ID:{cand['uid']}) {cand['desc']}" for cand in item['candidates']])
            prompt += f"[{i}] ë‹¨ì–´: '{item['word']}' -> í›„ë³´: [{options_str}]\n"
            
        prompt += """
        [ì¶œë ¥ ê·œì¹™]
        1. ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”. (ë§ˆí¬ë‹¤ìš´ ì—†ì´)
        2. KeyëŠ” ìœ„ ëª©ë¡ì˜ [ë²ˆí˜¸]ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. (ì˜ˆ: "0", "1")
        3. ValueëŠ” ì„ íƒí•œ ID ê°’ë§Œ ë„£ìœ¼ì„¸ìš”.
        4. ì˜ˆì‹œ: {"0": "272", "1": "677"}
        """
        
        raw_response = ""
        try:
            response = client.models.generate_content(
                model=model_name,
                contents=prompt,
                config={"response_mime_type": "application/json"}
            )
            raw_response = response.text
            
            clean_json_str = raw_response.replace('```json', '').replace('```', '').strip()
            if clean_json_str.endswith(',') or clean_json_str.endswith(',}'): 
                 clean_json_str = clean_json_str.rstrip(',}') + "}"
                 
            ai_data = json.loads(clean_json_str)
            return ai_data, raw_response

        except Exception as e:
            error_msg = f"Error: {e} | Raw: {raw_response}"
            return {}, error_msg

    def profile(self, tokens, sentence, client=None, model_name=None):
        """
        í˜•íƒœì†Œ ë¶„ì„ ê²°ê³¼(tokens)ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë“±ê¸‰ì„ í”„ë¡œíŒŒì¼ë§í•©ë‹ˆë‹¤.
        :param tokens: Kiwi í˜•íƒœì†Œ ë¶„ì„ ê²°ê³¼ (Token ê°ì²´ ë¦¬ìŠ¤íŠ¸ or dict ë¦¬ìŠ¤íŠ¸)
        :param sentence: ì›ë¬¸ ë¬¸ì¥ (AI ë¬¸ë§¥ íŒŒì•…ìš©)
        :param client: val (ë™ìŒì´ì˜ì–´ ì²˜ë¦¬ìš©)
        :param model_name: str
        :return: analysis_data (list), max_level (int), debug_log (str)
        """
        self.debug_lines = []
        max_level = 0
        analysis_data = []
        ambiguous_items = []
        
        self.debug_lines.append(f"ì…ë ¥: {sentence}")
        
        i = 0
        while i < len(tokens):
            # Token ê°ì²´ì¸ì§€ dictì¸ì§€ í™•ì¸ (ìœ ì—°ì„±)
            token = tokens[i]
            form = token.form if hasattr(token, 'form') else token['form']
            tag = token.tag if hasattr(token, 'tag') else token['tag']
            
            # ìœ„ì¹˜ ì •ë³´ (ì—†ì„ ìˆ˜ë„ ìˆìŒ)
            t_start = getattr(token, 'start', 0)
            t_len = getattr(token, 'len', 0)

            form_clean = self.data.clean_key(form)
            
            # 0. í‘œí˜„ íŒ¨í„´ ë§¤ì¹­
            idiom_matched = False
            if form_clean in self.data.idiom_map:
                candidates = self.data.idiom_map[form_clean]
                for cand in candidates:
                    seq = cand['sequence']
                    if i + len(seq) >= len(tokens): continue
                    match = True
                    matched_tokens_forms = [form]
                    for offset, target_stem in enumerate(seq):
                        next_t = tokens[i + 1 + offset]
                        next_form = next_t.form if hasattr(next_t, 'form') else next_t['form']
                        next_clean = self.data.clean_key(next_form)
                        if next_clean != target_stem: match = False; break
                        matched_tokens_forms.append(next_form)
                    
                    if match:
                        data = cand['data']
                        full_pattern_text = "+".join(matched_tokens_forms)
                        self.debug_lines.append(f"ğŸ§© í‘œí˜„ ë°œê²¬: {full_pattern_text} -> {data['desc']} (#{data['uid']})")
                        level_str = data['level']
                        if level_str:
                            try: max_level = max(max_level, int(re.sub(r'[^0-9]', '', str(level_str))))
                            except: pass
                        
                        last_t = tokens[i + len(seq)]
                        # ê¸¸ì´ ê³„ì‚° ì£¼ì˜ (Token ê°ì²´ì¼ ë•Œë§Œ ì •í™•)
                        full_len = 0
                        if hasattr(last_t, 'start'):
                            full_len = (last_t.start + last_t.len) - t_start

                        analysis_data.append({
                            "form": full_pattern_text, "tag_code": "Expression", "tag_name": "ë¬¸ë²•ì  í‘œí˜„",
                            "level": level_str, "id": f"í‘œí˜„#{data['uid']}", "desc": data['desc'],
                            "offset_start": t_start, "offset_len": full_len
                        })
                        i += (1 + len(seq))
                        idiom_matched = True; break
            if idiom_matched: continue

            # [VCP ì ˆëŒ€ ìš°ì„ ]
            if tag.startswith('VCP'):
                final_cand = self.data.ida_entry
                level_str = final_cand['level']
                self.debug_lines.append(f"ğŸ”’ ì§€ì •ì‚¬(VCP) ê°•ì œ ë§¤í•‘: ì´ë‹¤ -> {level_str} (#{final_cand['uid']})")
                if level_str:
                    try: max_level = max(max_level, int(re.sub(r'[^0-9]', '', str(level_str))))
                    except: pass
                analysis_data.append({
                    "form": form, "tag_code": tag, "tag_name": self.data.friendly_pos_map.get(tag, tag),
                    "level": level_str, "id": f"ë¬¸ë²•#{final_cand['uid']}", "desc": final_cand['desc'],
                    "offset_start": t_start, "offset_len": t_len
                })
                i += 1; continue 

            # 1. ë‹¨ì–´ ë³‘í•© (2-gram Lookahead)
            if i + 1 < len(tokens):
                next_token = tokens[i+1]
                next_form = next_token.form if hasattr(next_token, 'form') else next_token['form']
                next_tag = next_token.tag if hasattr(next_token, 'tag') else next_token['tag']

                combined_form = form_clean + self.data.clean_key(next_form)
                
                # ë³‘í•© ì‹œë„: (í•©ì¹œë‹¨ì–´, 'N') ë˜ëŠ” (í•©ì¹œë‹¨ì–´, 'V') ë“±ìœ¼ë¡œ ë°ì´í„° ì¡°íšŒ
                # ìš°ì„ ìˆœìœ„: ëª…ì‚¬(N) -> ë™ì‚¬(V) -> ê¸°íƒ€
                merge_found = False
                matched_candidate = None
                matched_pos_type = ''

                # [ì „ëµ] í•©ì¹œ í˜•íƒœê°€ ë°ì´í„°ë² ì´ìŠ¤ 'N'(ëª…ì‚¬) í˜¹ì€ 'V'(ë™ì‚¬) ë“±ì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                # ì˜ˆ: ì„ ìƒ(NNG) + ë‹˜(XSN) -> ì„ ìƒë‹˜(N) ì¡´ì¬ í™•ì¸
                pos_priorities = ['N', 'NB', 'V', 'M', 'MA', 'I']
                
                for p_key in pos_priorities:
                    # 1. ì›í˜• (ê·¸ëŒ€ë¡œ) ê²€ìƒ‰
                    lookup_keys = [combined_form]
                    
                    # 2. ë™ì‚¬/í‘œí˜„ ë“±ì¸ ê²½ìš° 'ë‹¤' ë¶™ì—¬ì„œ ê²€ìƒ‰ (ì–´ì§€ -> ì–´ì§€ë‹¤)
                    if p_key in ['V', 'ETC']: 
                         if not combined_form.endswith('ë‹¤'):
                             lookup_keys.append(combined_form + 'ë‹¤')

                    for key_var in lookup_keys:

                        # [FIX] word_mapê³¼ grammar_map ëª¨ë‘ ì¡°íšŒ
                        # 'ì–´ì§€ë‹¤' ê°™ì€ ë¬¸ë²•ì  í‘œí˜„ì´ë‚˜ ë™ì‚¬ëŠ” grammar_mapì— 'V' í‚¤ë¡œ ìˆì„ ìˆ˜ ìˆìŒ
                        candidates = []
                        if (key_var, p_key) in self.data.word_map:
                            candidates.extend(self.data.word_map[(key_var, p_key)])
                        if (key_var, p_key) in self.data.grammar_map:
                            candidates.extend(self.data.grammar_map[(key_var, p_key)])

                        main_cands = [c for c in candidates if c.get('is_main', False)]
                        if main_cands: candidates = main_cands
                        
                        if candidates:
                                # ë³‘í•© ì„±ê³µ
                                matched_candidate = candidates[0]
                                matched_pos_type = p_key
                                
                                # ë§Œì•½ 'ë‹¤'ë¥¼ ë¶™ì—¬ì„œ ì°¾ì•˜ë‹¤ë©´, í˜•íƒœë„ ê·¸ì— ë§ì¶”ê±°ë‚˜ ë©”ëª¨
                                # ì—¬ê¸°ì„œëŠ” combined_form ìì²´ëŠ” í•©ì¹œ í…ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ ë‘ê³ ,
                                # descë‚˜ idëŠ” ì°¾ì€ 'ì–´ì§€ë‹¤'ì˜ ê²ƒì„ ì‚¬ìš©í•¨.
                                
                                if len(candidates) > 1:
                                    ambiguous_items.append({
                                        'index': len(analysis_data), 
                                        'word': key_var, 
                                        'candidates': candidates
                                    })
                                merge_found = True
                                break
                    if merge_found: break
                
                # 'í•˜ë‹¤' íŒŒìƒ ìš©ì–¸ì˜ ê²½ìš° ì¶”ê°€ ì²˜ë¦¬ (ì–´ê·¼ ë³‘í•© ë¡œì§ ìœ ì§€)
                if not merge_found:
                    is_root_merge = (tag == 'XR' and next_tag in ['XSA', 'XSV', 'XSA-I', 'XSV-I'])
                    if is_root_merge:
                         combined_form_v = combined_form + 'ë‹¤'
                         if (combined_form_v, 'V') in self.data.word_map:
                             candidates = self.data.word_map[(combined_form_v, 'V')]
                             if candidates:
                                 matched_candidate = candidates[0]
                                 matched_pos_type = 'V'
                                 combined_form = combined_form_v # í¼ ì—…ë°ì´íŠ¸
                                 merge_found = True

                if merge_found and matched_candidate:
                    level_str = matched_candidate['level']
                    self.debug_lines.append(f"ğŸ”„ 2-gram ë³‘í•© ì„±ê³µ: {form}+{next_form} -> {combined_form} ({matched_pos_type}) -> {level_str}")
                    
                    if level_str:
                        try: max_level = max(max_level, int(re.sub(r'[^0-9]', '', str(level_str))))
                        except: pass
                    
                    # ê¸¸ì´ ê³„ì‚°
                    next_len = getattr(next_token, 'len', 0)
                    next_start = getattr(next_token, 'start', 0)
                    calc_len = (next_start + next_len) - t_start if next_start > 0 else 0

                    # [NEW] í’ˆì‚¬ ëª…ì¹­ ë™ì  ê²°ì •
                    pos_label = "ë³µí•©ì–´/íŒŒìƒì–´"
                    if 'class' in matched_candidate:
                        # ë¬¸ë²• DB ìœ ë˜
                        cls_val = matched_candidate['class']
                        if 'í‘œí˜„' in cls_val: pos_label = "ë¬¸ë²•ì  í‘œí˜„"
                        else: pos_label = cls_val
                    elif 'raw_pos' in matched_candidate:
                        # ë‹¨ì–´ DB ìœ ë˜
                        pos_label = matched_candidate['raw_pos']

                    analysis_data.append({
                        "form": combined_form,
                        "tag_code": f"{tag}+{next_tag}",
                        "tag_name": pos_label,
                        "level": level_str,
                        "id": f"ë‹¨ì–´#{matched_candidate['uid']}",
                        "desc": matched_candidate['desc'],
                        "offset_start": t_start,
                        "offset_len": calc_len
                    })
                    i += 2; continue

            # 2. ë‹¨ì¼ í† í° ì²˜ë¦¬
            source_type = ""; search_key = ""; candidates = []
            pos_key = self.data.pos_map.get(tag, 'ETC')
            # [FIX] ê¸°ë³¸ê°’ ì´ˆê¸°í™”
            target = form_clean 

            if tag in ['XSV', 'XSA'] and form_clean == 'í•˜':
                source_type = "ë‹¨ì–´"; candidates = [{'level': '2ê¸‰', 'uid': '1769', 'desc': 'ê±´ê°•í•˜ë‹¤', 'is_main': True}]
            elif tag in ['EF'] and form_clean == 'ë‹¤':
                source_type = "ë¬¸ë²•"; candidates = [{'level': '3ê¸‰', 'uid': '120', 'desc': '', 'is_main': True}]
            elif tag.startswith('J') or tag.startswith('E'):
                source_type = "ë¬¸ë²•"
                if (form_clean, pos_key) in self.data.grammar_map:
                    candidates = self.data.grammar_map[(form_clean, pos_key)]
                    search_key = f"({form_clean}, {pos_key})"
                else:
                    fallback_key = 'J' if tag.startswith('J') else 'E'
                    if (form_clean, fallback_key) in self.data.grammar_map:
                        candidates = self.data.grammar_map[(form_clean, fallback_key)]
                        search_key = f"({form_clean}, {fallback_key})"
            else:
                source_type = "ë‹¨ì–´"
                target = form_clean + 'ë‹¤' if pos_key == 'V' and not form_clean.endswith('ë‹¤') else form_clean
                search_key = f"({target}, {pos_key})"
                word_candidates = self.data.word_map.get((target, pos_key), [])
                grammar_candidates = []
                if (target, pos_key) in self.data.grammar_map:
                    grammar_candidates = self.data.grammar_map[(target, pos_key)]
                candidates = word_candidates + grammar_candidates

            final_level = "-"; final_id = ""; final_desc = ""
            if candidates:
                main_cands = [c for c in candidates if c.get('is_main', False)]
                if main_cands: candidates = main_cands

                if len(candidates) > 1:
                     ambiguous_items.append({'index': len(analysis_data), 'word': target, 'candidates': candidates})
                
                candidates.sort(key=lambda x: x['level'])
                sel = candidates[0]
                final_level = sel['level']; final_id = sel['uid']; final_desc = sel.get('desc', '') or sel.get('meaning', '')
                self.debug_lines.append(f"['{form}'({tag})] -> í‚¤:{search_key} -> ê²°ê³¼:{final_level} (#{final_id})")
                if final_level:
                    try: max_level = max(max_level, int(re.sub(r'[^0-9]', '', str(final_level))))
                    except: pass
            else:
                self.debug_lines.append(f"['{form}'({tag})] -> ê²€ìƒ‰ ì‹¤íŒ¨ (X)")

            analysis_data.append({
                "form": form, "tag_code": tag, "tag_name": self.data.friendly_pos_map.get(tag, tag),
                "level": final_level, "id": f"{source_type}#{final_id}" if final_id else "-",
                "desc": final_desc,
                "offset_start": t_start, "offset_len": t_len
            })
            i += 1
            
        # AI ê²°ê³¼ ë°˜ì˜ (ë™ìŒì´ì˜ì–´ ë¶„ì„)
        if ambiguous_items and client:
            self.debug_lines.append(f"ğŸ¤– AI ë™ìŒì´ì˜ì–´ ë¶„ì„ ì‹œì‘ ({len(ambiguous_items)}ê±´)...")
            ai_decisions, raw_log = self._disambiguate_with_ai(client, model_name, sentence, ambiguous_items)
            
            for i, item in enumerate(ambiguous_items):
                key_idx = str(i)
                word_key = item['word']
                target_idx = item['index']
                selected_uid = None
                
                if key_idx in ai_decisions:
                    selected_uid = str(ai_decisions[key_idx])
                elif word_key in ai_decisions:
                    selected_uid = str(ai_decisions[word_key])
                
                if selected_uid:
                    found = next((c for c in item['candidates'] if str(c['uid']) == selected_uid), None)
                    if found:
                        analysis_data[target_idx]['level'] = found['level']
                        analysis_data[target_idx]['id'] = f"ë‹¨ì–´#{found['uid']}" 
                        analysis_data[target_idx]['desc'] = f"ğŸ¤– {found['desc']}" 
                        self.debug_lines.append(f"âœ… AI êµì • [{item['word']}]: {found['desc']} (#{selected_uid})")
                        try: 
                            new_lvl = int(re.sub(r'[^0-9]', '', str(found['level'])))
                            max_level = max(max_level, new_lvl)
                        except: pass
                    else:
                        self.debug_lines.append(f"âš ï¸ ID ë¶ˆì¼ì¹˜: AIê°€ ì—†ëŠ” ID({selected_uid}) ë°˜í™˜")
                else:
                    self.debug_lines.append(f"âš ï¸ AI ì‘ë‹µ ëˆ„ë½ [{i}]: {item['word']}")

        return analysis_data, max_level, "\n".join(self.debug_lines)
