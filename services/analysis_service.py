import re
import json
import google.generativeai as genai
from config import Config
from services.morph_service import MorphService
from services.data_service import DataService

class AnalysisService:
    def __init__(self):
        self.morph = MorphService()
        self.data = DataService()
        self.model = None
        self._init_ai()
    
    def _init_ai(self):
        api_key = Config.GOOGLE_API_KEY
        if api_key:
            try:
                genai.configure(api_key=api_key)
                self.model = genai.GenerativeModel(
                    "models/gemini-2.0-flash-lite-preview-02-05",
                    generation_config={"response_mime_type": "application/json"}
                )
            except Exception as e:
                print(f"âš ï¸ AnalysisService AI Init Failed: {e}")

    def _disambiguate_with_ai(self, sentence, ambiguous_items):
        if not self.model or not ambiguous_items: return {}, "AI ë¯¸ì‚¬ìš©"
        
        prompt = f"""
        ë‹¹ì‹ ì€ í•œêµ­ì–´ ì–´íœ˜ íŒë…ê¸°ì…ë‹ˆë‹¤. ì•„ë˜ ë¬¸ë§¥ì„ ë³´ê³  ë™ìŒì´ì˜ì–´ ì¤‘ ê°€ì¥ ì ì ˆí•œ ì˜ë¯¸ë¥¼ ê³ ë¥´ì„¸ìš”.
        ë¬¸ë§¥: "{sentence}"
        
        [íŒë… ëŒ€ìƒ ëª©ë¡]
        """
        for i, item in enumerate(ambiguous_items):
            options_str = ", ".join([f"(ID:{cand['uid']}) {cand['desc']}" for cand in item['candidates']])
            prompt += f"[{i}] ë‹¨ì–´: '{item['word']}' -> í›„ë³´: [{options_str}]\n"
            
        prompt += """
        [ì¶œë ¥ ê·œì¹™]
        1. ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”. (ë§ˆí¬ë‹¤ìš´ ì—†ì´)
        2. KeyëŠ” ìœ„ ëª©ë¡ì˜ [ë²ˆí˜¸]ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. (ì˜ˆ: "0", "1")
        3. ValueëŠ” ì„ íƒí•œ ID ê°’ë§Œ ë„£ìœ¼ì„¸ìš”.
        4. ì˜ˆì‹œ: {"0": "272", "1": "677"}
        """
        
        raw_response = ""
        try:
            response = self.model.generate_content(prompt)
            raw_response = response.text
            
            clean_json_str = raw_response.replace('```json', '').replace('```', '').strip()
            if clean_json_str.endswith(',') or clean_json_str.endswith(',}'): 
                 clean_json_str = clean_json_str.rstrip(',}') + "}"
                 
            ai_data = json.loads(clean_json_str)
            return ai_data, raw_response

        except Exception as e:
            error_msg = f"Error: {e} | Raw: {raw_response}"
            return {}, error_msg

    def get_sentence_grade(self, sentence: str):
        if not self.data.is_ready: return "íŒë… ë¶ˆê°€", [], "ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨"
        if self.morph.use_mock or not self.morph.analyzer: return "ë¶„ì„ ë¶ˆê°€", [], "Kiwi ë¡œë“œ ì‹¤íŒ¨"
        
        try:
            res = self.morph.analyze(sentence)
            tokens = res[0][0]
        except Exception as e: return "ë¶„ì„ ì—ëŸ¬", [], f"Kiwi ë¶„ì„ ì˜¤ë¥˜: {str(e)}"

        max_level = 0; analysis_data = []; debug_lines = []
        ambiguous_items = [] 
        
        debug_lines.append(f"ì…ë ¥: {sentence}")
        
        i = 0
        while i < len(tokens):
            token = tokens[i]
            form = token.form; tag = token.tag; form_clean = self.data.clean_key(form)
            
            t_start = token.start
            t_len = token.len
            
            # 0. í‘œí˜„ íŒ¨í„´ ë§¤ì¹­
            idiom_matched = False
            if form_clean in self.data.idiom_map:
                candidates = self.data.idiom_map[form_clean]
                for cand in candidates:
                    seq = cand['sequence']
                    if i + len(seq) >= len(tokens): continue
                    match = True
                    matched_tokens_forms = [form]
                    for offset, target_stem in enumerate(seq):
                        next_t = tokens[i + 1 + offset]
                        next_clean = self.data.clean_key(next_t.form)
                        if next_clean != target_stem: match = False; break
                        matched_tokens_forms.append(next_t.form)
                    
                    if match:
                        data = cand['data']
                        full_pattern_text = "+".join(matched_tokens_forms)
                        debug_lines.append(f"ğŸ§© í‘œí˜„ ë°œê²¬: {full_pattern_text} -> {data['desc']} (#{data['uid']})")
                        level_str = data['level']
                        if level_str:
                            try: max_level = max(max_level, int(re.sub(r'[^0-9]', '', str(level_str))))
                            except: pass
                        
                        last_t = tokens[i + len(seq)]
                        full_len = (last_t.start + last_t.len) - t_start

                        analysis_data.append({
                            "form": full_pattern_text, "tag_code": "Expression", "tag_name": "ë¬¸ë²•ì  í‘œí˜„",
                            "level": level_str, "id": f"í‘œí˜„#{data['uid']}", "desc": data['desc'],
                            "offset_start": t_start, "offset_len": full_len
                        })
                        i += (1 + len(seq))
                        idiom_matched = True; break
            if idiom_matched: continue

            # [VCP ì ˆëŒ€ ìš°ì„ ]
            if tag.startswith('VCP'):
                final_cand = self.data.ida_entry
                level_str = final_cand['level']
                debug_lines.append(f"ğŸ”’ ì§€ì •ì‚¬(VCP) ê°•ì œ ë§¤í•‘: ì´ë‹¤ -> {level_str} (#{final_cand['uid']})")
                if level_str:
                    try: max_level = max(max_level, int(re.sub(r'[^0-9]', '', str(level_str))))
                    except: pass
                analysis_data.append({
                    "form": form, "tag_code": tag, "tag_name": self.data.friendly_pos_map.get(tag, tag),
                    "level": level_str, "id": f"ë¬¸ë²•#{final_cand['uid']}", "desc": final_cand['desc'],
                    "offset_start": t_start, "offset_len": t_len
                })
                i += 1; continue 

            # 1. ë‹¨ì–´ ë³‘í•©
            if i + 1 < len(tokens):
                next_token = tokens[i+1]
                curr_pos_type = self.data.pos_map.get(tag, 'ETC')
                next_pos_type = self.data.pos_map.get(next_token.tag, 'ETC')
                
                is_noun_merge = (curr_pos_type in ['N', 'NB'] and next_pos_type in ['N', 'NB'])
                is_root_merge = (tag == 'XR' and next_token.tag in ['XSA', 'XSV', 'XSA-I', 'XSV-I'])

                if is_noun_merge or is_root_merge:
                    suffix = 'ë‹¤' if is_root_merge else ''
                    combined_form = form_clean + self.data.clean_key(next_token.form) + suffix
                    target_pos = 'V' if is_root_merge else 'N'
                    
                    if (combined_form, target_pos) in self.data.word_map:
                        merged_cands = self.data.word_map[(combined_form, target_pos)]
                        
                        main_cands = [c for c in merged_cands if c.get('is_main', False)]
                        if main_cands: merged_cands = main_cands

                        if len(merged_cands) > 1:
                            ambiguous_items.append({'index': len(analysis_data), 'word': combined_form, 'candidates': merged_cands})
                        
                        final_cand = merged_cands[0] 
                        level_str = final_cand['level']
                        debug_lines.append(f"ğŸ”„ ë³‘í•© ì„±ê³µ: {combined_form} ({target_pos}) -> {level_str}")
                        if level_str:
                            try: max_level = max(max_level, int(re.sub(r'[^0-9]', '', str(level_str))))
                            except: pass
                        
                        pos_label = "ë™ì‚¬/í˜•ìš©ì‚¬(íŒŒìƒ)" if is_root_merge else "ë³µí•©ì–´"
                        
                        analysis_data.append({
                            "form": combined_form, "tag_code": f"{tag}+{next_token.tag}", "tag_name": pos_label,
                            "level": level_str, "id": f"ë‹¨ì–´#{final_cand['uid']}", "desc": final_cand['desc'],
                            "offset_start": t_start, "offset_len": (next_token.start + next_token.len) - t_start
                        })
                        i += 2; continue

            # 2. ë‹¨ì¼ í† í° ì²˜ë¦¬
            source_type = ""; search_key = ""; candidates = []
            pos_key = self.data.pos_map.get(tag, 'ETC')
            # [FIX] ê¸°ë³¸ê°’ ì´ˆê¸°í™”
            target = form_clean 

            if tag in ['XSV', 'XSA'] and form_clean == 'í•˜':
                source_type = "ë‹¨ì–´"; candidates = [{'level': '2ê¸‰', 'uid': '1769', 'desc': 'ê±´ê°•í•˜ë‹¤', 'is_main': True}]
            elif tag in ['EF'] and form_clean == 'ë‹¤':
                source_type = "ë¬¸ë²•"; candidates = [{'level': '3ê¸‰', 'uid': '120', 'desc': '', 'is_main': True}]
            elif tag.startswith('J') or tag.startswith('E'):
                source_type = "ë¬¸ë²•"
                if (form_clean, pos_key) in self.data.grammar_map:
                    candidates = self.data.grammar_map[(form_clean, pos_key)]
                    search_key = f"({form_clean}, {pos_key})"
                else:
                    fallback_key = 'J' if tag.startswith('J') else 'E'
                    if (form_clean, fallback_key) in self.data.grammar_map:
                        candidates = self.data.grammar_map[(form_clean, fallback_key)]
                        search_key = f"({form_clean}, {fallback_key})"
            else:
                source_type = "ë‹¨ì–´"
                target = form_clean + 'ë‹¤' if pos_key == 'V' and not form_clean.endswith('ë‹¤') else form_clean
                search_key = f"({target}, {pos_key})"
                word_candidates = self.data.word_map.get((target, pos_key), [])
                grammar_candidates = []
                if (target, pos_key) in self.data.grammar_map:
                    grammar_candidates = self.data.grammar_map[(target, pos_key)]
                candidates = word_candidates + grammar_candidates

            final_level = "-"; final_id = ""; final_desc = ""
            if candidates:
                main_cands = [c for c in candidates if c.get('is_main', False)]
                if main_cands: candidates = main_cands

                if len(candidates) > 1:
                     ambiguous_items.append({'index': len(analysis_data), 'word': target, 'candidates': candidates})
                
                candidates.sort(key=lambda x: x['level'])
                sel = candidates[0]
                final_level = sel['level']; final_id = sel['uid']; final_desc = sel.get('desc', '') or sel.get('meaning', '')
                debug_lines.append(f"['{form}'({tag})] -> í‚¤:{search_key} -> ê²°ê³¼:{final_level} (#{final_id})")
                if final_level:
                    try: max_level = max(max_level, int(re.sub(r'[^0-9]', '', str(final_level))))
                    except: pass
            else:
                debug_lines.append(f"['{form}'({tag})] -> ê²€ìƒ‰ ì‹¤íŒ¨ (X)")

            analysis_data.append({
                "form": form, "tag_code": tag, "tag_name": self.data.friendly_pos_map.get(tag, tag),
                "level": final_level, "id": f"{source_type}#{final_id}" if final_id else "-",
                "desc": final_desc,
                "offset_start": t_start, "offset_len": t_len
            })
            i += 1

        # AI ê²°ê³¼ ë°˜ì˜
        if ambiguous_items and self.model:
            debug_lines.append(f"ğŸ¤– AI ë™ìŒì´ì˜ì–´ íŒë… ì‹œì‘ ({len(ambiguous_items)}ê±´)...")
            ai_decisions, raw_log = self._disambiguate_with_ai(sentence, ambiguous_items)
            
            for i, item in enumerate(ambiguous_items):
                key_idx = str(i)
                word_key = item['word']
                target_idx = item['index']
                selected_uid = None
                
                if key_idx in ai_decisions:
                    selected_uid = str(ai_decisions[key_idx])
                elif word_key in ai_decisions:
                    selected_uid = str(ai_decisions[word_key])
                
                if selected_uid:
                    found = next((c for c in item['candidates'] if str(c['uid']) == selected_uid), None)
                    if found:
                        analysis_data[target_idx]['level'] = found['level']
                        analysis_data[target_idx]['id'] = f"ë‹¨ì–´#{found['uid']}" 
                        analysis_data[target_idx]['desc'] = f"ğŸ¤– {found['desc']}" 
                        debug_lines.append(f"âœ… AI êµì • [{item['word']}]: {found['desc']} (#{selected_uid})")
                        try: 
                            new_lvl = int(re.sub(r'[^0-9]', '', str(found['level'])))
                            max_level = max(max_level, new_lvl)
                        except: pass
                    else:
                        debug_lines.append(f"âš ï¸ ID ë¶ˆì¼ì¹˜: AIê°€ ì—†ëŠ” ID({selected_uid}) ë°˜í™˜")
                else:
                    debug_lines.append(f"âš ï¸ AI ì‘ë‹µ ëˆ„ë½ [{i}]: {item['word']}")

        final_grade = f"{max_level}ê¸‰" if max_level > 0 else "íŒë³„ ë¶ˆê°€"
        return final_grade, analysis_data, "\n".join(debug_lines)

    def analyze_morphs(self, sentence):
        if not self.morph.analyzer: return []
        res = self.morph.analyze(sentence)
        tokens = res[0][0]
        return [{'form': t.form, 'tag': t.tag} for t in tokens]

    def get_visualization_data(self, analysis_result, sentence):
        grade_counts = {f"{i}ê¸‰": 0 for i in range(1, 7)}
        text_segments = []
        
        for item in analysis_result:
            lvl = item.get('level', '')
            if 'ê¸‰' in lvl:
                full_lvl_str = re.sub(r'[^0-9]', '', lvl)
                if full_lvl_str and f"{full_lvl_str}ê¸‰" in grade_counts:
                    grade_counts[f"{full_lvl_str}ê¸‰"] += 1

        visualization_data = {
            "labels": [k for k, v in grade_counts.items() if v > 0],
            "data": [v for v in grade_counts.values() if v > 0]
        }
        
        for i, item in enumerate(analysis_result):
            item['_ui_id'] = f"seg-{i}-{item.get('offset_start', 0)}"
        
        sorted_analysis = sorted(analysis_result, key=lambda x: x.get('offset_start', -1))
        current_cursor = 0
        
        for item in sorted_analysis:
            start = item.get('offset_start')
            length = item.get('offset_len')
            
            if start is None or length is None: continue
            
            if start > current_cursor:
                text_segments.append({
                    "text": sentence[current_cursor:start],
                    "type": "plain"
                })
            
            grade_class = "text-grade-none"
            lvl = item.get('level', '')
            if 'ê¸‰' in lvl:
                num = re.sub(r'[^0-9]', '', lvl)
                if num: grade_class = f"text-grade-{num}"
            
            text_segments.append({
                "text": item['form'],
                "type": "graded",
                "class": grade_class,
                "info": item 
            })
            
            current_cursor = max(current_cursor, start + length)
            
        if current_cursor < len(sentence):
            text_segments.append({
                "text": sentence[current_cursor:],
                "type": "plain"
            })
            
        return visualization_data, text_segments
